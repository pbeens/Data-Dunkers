{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from io import StringIO\n",
    "\n",
    "# Define the range of seasons to process\n",
    "start_year = 2025  \n",
    "end_year = 2025    \n",
    "\n",
    "# Initialize WebDriver (Ensure geckodriver is installed and in your PATH)\n",
    "wd = webdriver.Firefox()\n",
    "\n",
    "def scrape_season(year):\n",
    "    \"\"\"Scrapes NBA player stats for a given season and saves to CSV, skipping empty seasons.\"\"\"\n",
    "    url = f\"https://www.espn.com/nba/stats/player/_/season/{year}/seasontype/2\"\n",
    "    filename = f'nba_player_stats_{year-1}-{year}.csv'\n",
    "    \n",
    "    print(f\"Processing season {year-1}-{year}...\")\n",
    "\n",
    "    # Open the URL\n",
    "    wd.get(url)\n",
    "    time.sleep(3)  # Allow page to load\n",
    "\n",
    "    # Check if \"No Data Available\" message is present\n",
    "    try:\n",
    "        no_data_message = wd.find_element(By.XPATH, \"//div[contains(text(), 'No Data Available')]\")\n",
    "        if no_data_message:\n",
    "            print(f\"Skipping {year-1}-{year}: No Data Available\")\n",
    "            return  # Skip this season\n",
    "    except NoSuchElementException:\n",
    "        pass  # No error means data is available, continue scraping\n",
    "\n",
    "    # Function to click \"Show More\" until all data is loaded\n",
    "    def click_show_more():\n",
    "        while True:\n",
    "            try:\n",
    "                # Locate the \"Show More\" link inside the div\n",
    "                show_more_link = wd.find_element(By.XPATH, \"//div[contains(@class, 'loadMore')]//a[contains(@class, 'loadMore__link')]\")\n",
    "                \n",
    "                # Scroll into view\n",
    "                wd.execute_script(\"arguments[0].scrollIntoView();\", show_more_link)\n",
    "                time.sleep(1)  # Allow scrolling time\n",
    "                \n",
    "                # Click using JavaScript (ensures it works)\n",
    "                wd.execute_script(\"arguments[0].click();\", show_more_link)\n",
    "                time.sleep(2)  # Allow content to load\n",
    "            except NoSuchElementException:\n",
    "                print(\"No more 'Show More' button found. Page is fully loaded.\")\n",
    "                break\n",
    "            except ElementClickInterceptedException:\n",
    "                print(\"Click intercepted. Retrying after a short wait...\")\n",
    "                time.sleep(2)\n",
    "\n",
    "    # Click \"Show More\" until everything is loaded\n",
    "    click_show_more()\n",
    "\n",
    "    # Extract player names\n",
    "    try:\n",
    "        player_elements = wd.find_elements(By.XPATH, \"//tr[contains(@class, 'Table__TR')]//td[2]//a\")\n",
    "        names = [element.text for element in player_elements if element.text]\n",
    "        print(f\"Number of player names extracted: {len(names)}\")\n",
    "        if len(names) == 0:\n",
    "            print(f\"Skipping {year-1}-{year}: No player data found.\")\n",
    "            return  # Skip this season\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {year-1}-{year}: Error extracting player names:\", e)\n",
    "        return  # Skip this season\n",
    "\n",
    "    # Extract tables using pandas (fixing the warning)\n",
    "    html_source = wd.page_source\n",
    "    try:\n",
    "        tables = pd.read_html(StringIO(html_source))\n",
    "    except ValueError:\n",
    "        print(f\"Skipping {year-1}-{year}: No tables found.\")\n",
    "        return  # Skip this season\n",
    "\n",
    "    # Ensure tables exist\n",
    "    if len(tables) < 2:\n",
    "        print(f\"Skipping {year-1}-{year}: Expected at least 2 tables but found {len(tables)}\")\n",
    "        return  # Skip this season\n",
    "\n",
    "    # Extract player data\n",
    "    players = tables[0]\n",
    "    stats = tables[1]\n",
    "\n",
    "    # Drop 'RK' column if it exists\n",
    "    if 'RK' in players.columns:\n",
    "        players = players.drop(columns=['RK'])\n",
    "\n",
    "    # Add extracted player names\n",
    "    players['Name'] = names\n",
    "\n",
    "    # Merge player and stats DataFrames\n",
    "    df = pd.concat([players, stats], axis=1)\n",
    "\n",
    "    # Display the final DataFrame\n",
    "    print(df.head())\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Loop through each season\n",
    "for year in range(start_year, end_year + 1):\n",
    "    scrape_season(year)\n",
    "\n",
    "# Close WebDriver after processing all seasons\n",
    "wd.quit()\n",
    "print(\"All seasons processed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
